# -*- coding: utf-8 -*-
"""back_end.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-1zdQkrXTsqDjlPalJhseem3VEHk1nWj
"""

import sys
from collections import Counter, OrderedDict
import itertools
from itertools import islice, count, groupby
import pandas as pd
import os
import re
from operator import itemgetter
import nltk
from nltk.stem.porter import *
from nltk.corpus import stopwords
from time import time
from timeit import timeit
from pathlib import Path
import pickle
import pandas as pd
import numpy as np
from google.cloud import storage

# from google.colab import auth
# auth.authenticate_user()

import pickle
from google.cloud import storage

# Initialize the client
client = storage.Client()

# Specify your bucket name
bucket_name = 'jhonny_328783105'
# Directory within the bucket where your index file is located
directory_name = 'title_index/'

# Placeholder for your index object
title_index = None

# List blobs in the specified directory
blobs = client.list_blobs(bucket_name, prefix=directory_name)

# Loop through the blobs to find the index file
for blob in blobs:
    # Assuming your index file is named 'index' and stored as a pickle
    if blob.name.endswith('index.pkl'):
        # Download the blob to a local file
        with open("local_index.pkl", "wb") as file_obj:
            blob.download_to_file(file_obj)

        # Load the index from the downloaded file
        with open("local_index.pkl", "rb") as file_obj:
            title_index = pickle.load(file_obj)

        # Assuming the file is found and loaded, break out of the loop
        break

# Ensure the index object was loaded
if title_index is not None:
    # Access and print the 'self.term_total' property
    print(title_index.term_total)
else:
    print("Index file not found or failed to load.")


# Initialize the client
client = storage.Client()
bucket_name = 'jhonny_328783105'

# Specify your bucket name
# bucket_name = 'jhonny_328783105'
# Directory within the bucket where your index file is located
directory_name = 'body_index/'

# Placeholder for your index object
body_index = None

# List blobs in the specified directory
blobs = client.list_blobs(bucket_name, prefix=directory_name)

# Loop through the blobs to find the index file
for blob in blobs:
    # Assuming your index file is named 'index' and stored as a pickle
    if blob.name.endswith('index.pkl'):
        # Download the blob to a local file
        with open("local_index.pkl", "wb") as file_obj:
            blob.download_to_file(file_obj)

        # Load the index from the downloaded file
        with open("local_index.pkl", "rb") as file_obj:
            body_index = pickle.load(file_obj)

        # Assuming the file is found and loaded, break out of the loop
        break

# Ensure the index object was loaded
if body_index is not None:
    # Access and print the 'self.term_total' property
    print(body_index.term_total)
else:
    print("Index file not found or failed to load.")


from google.cloud import storage
import pickle
import io

def load_pickle_from_gcs(bucket_name, source_blob_name):
    """Loads a pickle file from a Google Cloud Storage bucket into a Python dictionary."""
    # Create a GCS client
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)

    # Create a blob object
    blob = bucket.blob(source_blob_name)

    # Download the pickle file as bytes
    pickle_bytes = blob.download_as_bytes()

    # Use BytesIO to convert bytes back into a file-like object
    pickle_data = io.BytesIO(pickle_bytes)

    # Load the dictionary from the pickle data
    data = pickle.load(pickle_data)

    return data

# Usage
# bucket_name = 'your-bucket-name'
source_blob_name = 'PageRank_dict.pkl'
loaded_dict = load_pickle_from_gcs(bucket_name, source_blob_name)

doc_id_to_title_dict = load_pickle_from_gcs(bucket_name, 'title_id_dict/dict_id_to_title.pkl')

PageRank_Dict = dict(loaded_dict)

'''Version without threads'''

# from collections import defaultdict, Counter
#
# def bm25_score(index_title, index_body, query, k1=1.5, b=0.75, k3=1.5, title_weight=3.0, body_weight=2.0):
#     """
#     Calculate BM25 score for a given query using instances of the InvertedIndex class
#     for both title and body indices. This version of the function weights the scores
#     from title and body differently based on provided weights.
#
#     Parameters:
#     - index_title: An instance of the InvertedIndex class for titles.
#     - index_body: An instance of the InvertedIndex class for bodies.
#     - query: A string representing the search query.
#     - bucket_name: Name of the Google Cloud Storage bucket.
#     - k1, b, k3: BM25 parameters.
#     - title_weight: The weight for the BM25 scores from the title index.
#     - body_weight: The weight for the BM25 scores from the body index.
#
#     Returns:
#     A dictionary of document IDs and their weighted BM25 scores.
#     """
#     score = defaultdict(float)
#     query_terms = Counter(query)  # Assuming query is a string of space-separated terms
#
#     # Score from titles
#     for term, freq in query_terms.items():
#         if term in index_title.df:
#             for doc_id, bm25 in index_title.read_a_posting_list('', term, bucket_name):
#                 score[doc_id] += (bm25 * freq / (freq + k3)) * title_weight
#         if term in index_body.df:
#             for doc_id, bm25 in index_body.read_a_posting_list('', term, bucket_name):
#                 score[doc_id] += (bm25 * freq / (freq + k3)) * body_weight
#     # Score from bodies
#
#
#     return dict(score)

'''Version with threads'''

from collections import defaultdict, Counter
from concurrent.futures import ThreadPoolExecutor, as_completed

def bm25_score(index_title, index_body, query_terms, k1=1.5, b=0.75, k3=1.5, title_weight=3.0, body_weight=2.0):
    """
    Calculate BM25 score for a given query using parallel processing with threads.
    Weights the scores from title and body differently based on provided weights.

    Parameters:
    - index_title: An instance of the InvertedIndex class for titles.
    - index_body: An instance of the InvertedIndex class for bodies.
    - query_terms: A list of stemmed tokens representing the search query.
    - k1, b, k3: BM25 parameters.
    - title_weight: The weight for the BM25 scores from the title index.
    - body_weight: The weight for the BM25 scores from the body index.

    Returns:
    A dictionary of document IDs and their weighted BM25 scores.
    """
    score = defaultdict(float)
    term_frequencies = Counter(query_terms)  # Use the list of tokens directly

    def process_term(term, index, weight, bucket_name):
        local_score = defaultdict(float)
        if term in index.df:
            for doc_id, bm25 in index.read_a_posting_list('', term, bucket_name):
                # Calculate frequency from Counter and apply BM25 formula
                term_frequency = term_frequencies[term]
                local_score[doc_id] += (bm25 * term_frequency / (term_frequency + k3)) * weight
        return local_score

    # Define bucket names for title and body
    # title_bucket_name = 'title_bucket'
    # body_bucket_name = 'body_bucket'

    with ThreadPoolExecutor() as executor:
        futures = []
        for term in term_frequencies:
            futures.append(executor.submit(process_term, term, index_title, title_weight, bucket_name))
            futures.append(executor.submit(process_term, term, index_body, body_weight, bucket_name))

        for future in as_completed(futures):
            local_score = future.result()
            for doc_id, term_score in local_score.items():
                score[doc_id] += term_score

    return dict(score)


def combine_bm25_pagerank(bm25_scores, pagerank_dict, bm25_weight=0.5, pagerank_weight=0.5):
    """
    Combine BM25 scores with PageRank scores using a weighted average.

    Parameters:
    - bm25_scores: A dictionary of document IDs and their BM25 scores.
    - pagerank_dict: A dictionary of document IDs and their PageRank scores.
    - bm25_weight: The weight for BM25 scores in the combined score.
    - pagerank_weight: The weight for PageRank scores in the combined score.

    Returns:
    A dictionary of document IDs and their combined BM25 and PageRank scores.
    """
    combined_scores = {}

    # Find max scores for normalization
    max_bm25 = max(bm25_scores.values())
    max_pr = max(pagerank_dict.values())

    # Combine the scores
    for doc_id in bm25_scores:
        normalized_bm25 = (bm25_scores[doc_id] / max_bm25) if max_bm25 else 0
        normalized_pr = (pagerank_dict.get(doc_id, 0) / max_pr) if max_pr else 0

        combined_score = (bm25_weight * normalized_bm25) + (pagerank_weight * normalized_pr)
        combined_scores[doc_id] = combined_score

    return combined_scores



# Assuming bm25_scores is a dictionary of BM25 scores from your function
# Assuming pagerank_dict is a dictionary where keys are doc_ids and values are PageRank scores
nltk.download('punkt')
nltk.download('stopwords')
english_stopwords = frozenset(stopwords.words('english'))
corpus_stopwords = ["category", "references", "also", "external", "links",
                    "may", "first", "see", "history", "people", "one", "two",
                    "part", "thumb", "including", "second", "following",
                    "many", "however", "would", "became"]

all_stopwords = english_stopwords.union(corpus_stopwords)
# Assuming 'index' is your InvertedIndex instance loaded with all necessary data
porter = PorterStemmer()


def back_end(query):
    tokens = nltk.word_tokenize(query)
    # Filter out non-word tokens and stem each token using Porter Stemmer
    stemmed_tokens = [porter.stem(token) for token in tokens if token.isalpha() and token not in all_stopwords]

    # Combine the BM25 and PageRank scores
    bm25_scores = bm25_score(title_index, body_index, stemmed_tokens)  # This is the function call you will make to get the BM25 scores
    sorted_scores = {k: v for k, v in sorted(bm25_scores.items(), key=lambda item: item[1], reverse=True)}

    # Using islice to slice the first 200 items
    bm25_scores_200 = dict(islice(sorted_scores.items(), 200))
    PageRank_Dict_200 = {k: PageRank_Dict[k] for k in bm25_scores_200 if k in PageRank_Dict}


    if len(bm25_scores) > 0 and len(PageRank_Dict_200) > 0:
        combined_scores = combine_bm25_pagerank(bm25_scores, PageRank_Dict_200)
        combined_scores_sorted = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
        return [(str(doc_id), doc_id_to_title_dict[doc_id]) for doc_id, score in combined_scores_sorted[:30]]
    else:
        return []
# def cross_validate(train_queries, weigths_bm25, kfolds=5):
#     """
#     Perform k-fold cross-validation to find the optimal BM25 parameters.
#     """
#     kf = KFold(n_splits=kfolds)
#     best_score = 0
#     best_params = {}


#     fold_scores = []
#     for weigths in weigths_bm25:

#       for train_index, test_index in kf.split(train_queries):
#           # Split queries into train/test for this fold
#           train_fold = {list(train_queries.keys())[i]: train_queries[list(train_queries.keys())[i]] for i in train_index}
#           test_fold = {list(train_queries.keys())[i]: train_queries[list(train_queries.keys())[i]] for i in test_index}
#           nltk.download('punkt')
#           nltk.download('stopwords')
#           english_stopwords = frozenset(stopwords.words('english'))
#           corpus_stopwords = ["category", "references", "also", "external", "links",
#                               "may", "first", "see", "history", "people", "one", "two",
#                               "part", "thumb", "including", "second", "following",
#                               "many", "however", "would", "became"]

#           all_stopwords = english_stopwords.union(corpus_stopwords)
#           # Assuming 'index' is your InvertedIndex instance loaded with all necessary data
#           porter = PorterStemmer()

#           tokens = nltk.word_tokenize(query)
#           # Filter out non-word tokens and stem each token using Porter Stemmer
#           stemmed_tokens = [porter.stem(token) for token in tokens if token.isalpha() and token not in all_stopwords]

#           bm25_scores = bm25_score(index_object, stemmed_tokens)
#           # Sort the scores in descending order to get documents with the highest scores first
#           # sorted_scores = sorted(bm25_scores.items(), key=lambda x: x[1], reverse=True)
#           sorted_scores = {k: v for k, v in sorted(bm25_scores.items(), key=lambda item: item[1], reverse=True)}

#           # Using islice to slice the first 200 items
#           bm25_scores_200 = dict(islice(sorted_scores.items(), 200))
#           # bm25_scores_200
#           # print(bm25_scores_200)
#           PageRank_Dict_200 = {k: PageRank_Dict[k] for k in bm25_scores_200 if k in PageRank_Dict}
#           print(PageRank_Dict_200)
#           # print(first_200_items)

#           combined_scores = combine_bm25_pagerank(bm25_scores, PageRank_Dict_200)

#           combined_scores_sorted = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)

#           # Here, implement your logic to calculate BM25 scores,
#           # combine with PageRank, and get the sorted document list.
#           # This might involve calling your `bm25_score` and
#           # `combine_bm25_pagerank` functions appropriately.

#           # For simplicity, assume we're directly calculating precision, recall, and F1 here.
#           # In practice, you'd calculate these based on the output of the above.
#           fold_precision, fold_recall, fold_f1 = evaluate_scores(...)

#           fold_scores.append(fold_f1)

#         avg_fold_score = sum(fold_scores) / len(fold_scores)
#         if avg_fold_score > best_score:
#             best_score = avg_fold_score
#             best_params = {'k1': k1, 'b': b, 'k3': k3}

#     return best_params
# weight_bm25_values = np.linspace(0.3,0.8, num=5)
# best_params = cross_validate(train_queries, weight_bm25_values)


