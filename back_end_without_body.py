# -*- coding: utf-8 -*-
"""back_end.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DdsQt_Cf94E-SibN52sW9ct2R40Tyha-
"""

import sys
from collections import Counter, OrderedDict
import itertools
from itertools import islice, count, groupby
import pandas as pd
import os
import re
from operator import itemgetter
import nltk
from nltk.stem.porter import *
from nltk.corpus import stopwords
from time import time
from timeit import timeit
from pathlib import Path
import pickle
import pandas as pd
import numpy as np
from google.cloud import storage


global index_object
global merged_dict




# from google.cloud import storage
# bucket_name = "jhonny_328783105"

import pickle

# Initialize the client
client = storage.Client()

# Specify your bucket name
bucket_name = 'jhonny_328783105'
# Directory within the bucket where your index file is located
directory_name = 'title_index/'

# Placeholder for your index object
index_object = None

# List blobs in the specified directory
blobs = client.list_blobs(bucket_name, prefix=directory_name)

# Loop through the blobs to find the index file
for blob in blobs:
    # Assuming your index file is named 'index' and stored as a pickle
    if blob.name.endswith('index.pkl'):
        # Download the blob to a local file
        with open("local_index.pkl", "wb") as file_obj:
            blob.download_to_file(file_obj)

        import pickle

        with open("local_index.pkl", "rb") as file_obj:
            index_object = pickle.load(file_obj)

        # Assuming the file is found and loaded, break out of the loop
        break

# Ensure the index object was loaded
if index_object is not None:
    # Access and print the 'self.term_total' property
    print(index_object.term_total)
else:
    print("Index file not found or failed to load.")

bucket = client.bucket(bucket_name)

import gcsfs

import pandas as pd

# Correct URI format using the directory and wildcard to match all 'part' files
# fs = gcsfs.GCSFileSystem(project='jhonny_328783105')
#
# # Correct URI format using the directory and wildcard to match all 'part' files
# uri = "gs://jhonny_328783105/doc_lengths.csv/part-*"
#
# # List all files in the bucket that match the pattern
# files = fs.glob(uri)
#
# # Read and concatenate all CSV files into a single DataFrame
# df = pd.concat([pd.read_csv(fs.open(file)) for file in files], ignore_index=True)
#
# # Convert the DataFrame into a dictionary
# doc_length_dict = dict(zip(df['doc_id'], df['doc_length']))





def load_pickle_from_gcs(bucket_name, source_blob_name):
    """Loads a pickle file from a Google Cloud Storage bucket into a Python dictionary."""
    # Create a GCS client
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)

    # Create a blob object
    blob = bucket.blob(source_blob_name)

    # Download the pickle file as bytes
    pickle_bytes = blob.download_as_bytes()

    # Use BytesIO to convert bytes back into a file-like object
    pickle_data = io.BytesIO(pickle_bytes)

    # Load the dictionary from the pickle data
    data = pickle.load(pickle_data)

    return data
import math
from collections import defaultdict
from google.cloud import storage
import pickle
import io

source_blob_name = 'PageRank_dict.pkl'
loaded_dict = load_pickle_from_gcs(bucket_name, source_blob_name)

doc_id_to_title_dict = load_pickle_from_gcs(bucket_name, 'title_id_dict/dict_id_to_title.pkl')


PageRank_Dict = dict(loaded_dict)
def bm25_score(index, query, k1=1.5, b=0.75, k3=1.5):
    """
    Calculate BM25 score for a given query using an InvertedIndex instance.

    Parameters:
    - index: An instance of the InvertedIndex class.
    - query: A string representing the search query.
    - k1, b: BM25 parameters.

    Returns:
    A dictionary of document IDs and their BM25 scores.
    """
    score = defaultdict(float)
    query_terms = Counter(query)

    for term, freq in query_terms.items():
        if term in index.df:
            for doc_id,bm25 in index.read_a_posting_list('', term, bucket_name):

                score[doc_id] += bm25 * freq / (freq + k3)


    return score


# Assuming 'index' is your InvertedIndex instance loaded with all necessary data


# Sort the scores in descending order to get documents with the highest scores first
from collections import Counter, defaultdict

def combine_bm25_pagerank(bm25_scores, pagerank_dict, bm25_weight=0.5, pagerank_weight=0.5):
    """
    Combine BM25 scores with PageRank scores using a weighted average.

    Parameters:
    - bm25_scores: A dictionary of document IDs and their BM25 scores.
    - pagerank_dict: A dictionary of document IDs and their PageRank scores.
    - bm25_weight: The weight for BM25 scores in the combined score.
    - pagerank_weight: The weight for PageRank scores in the combined score.

    Returns:
    A dictionary of document IDs and their combined BM25 and PageRank scores.
    """
    combined_scores = {}

    # Find max scores for normalization
    max_bm25 = max(bm25_scores.values())
    max_pr = max(pagerank_dict.values())

    # Combine the scores
    for doc_id in bm25_scores:
        normalized_bm25 = (bm25_scores[doc_id] / max_bm25) if max_bm25 else 0
        normalized_pr = (pagerank_dict.get(doc_id, 0) / max_pr) if max_pr else 0

        combined_score = (bm25_weight * normalized_bm25) + (pagerank_weight * normalized_pr)
        combined_scores[doc_id] = combined_score

    return combined_scores

import nltk

nltk.download('punkt')
nltk.download('stopwords')
english_stopwords = frozenset(stopwords.words('english'))
corpus_stopwords = ["category", "references", "also", "external", "links",
                    "may", "first", "see", "history", "people", "one", "two",
                    "part", "thumb", "including", "second", "following",
                    "many", "however", "would", "became"]

all_stopwords = english_stopwords.union(corpus_stopwords)
porter = PorterStemmer()
# Assuming 'index' is your InvertedIndex instance loaded with all necessary data
def backend_search(query):
    tokens = nltk.word_tokenize(query)

    stemmed_tokens = [porter.stem(token) for token in tokens if token.isalpha() and token not in all_stopwords]

    bm25_scores = bm25_score(index_object, stemmed_tokens)
    # Sort the scores in descending order to get documents with the highest scores first
    # sorted_scores = sorted(bm25_scores.items(), key=lambda x: x[1], reverse=True)
    sorted_scores = {k: v for k, v in sorted(bm25_scores.items(), key=lambda item: item[1], reverse=True)}

    # Using islice to slice the first 200 items
    bm25_scores_200 = dict(islice(sorted_scores.items(), 200))
    # bm25_scores_200
    # print(bm25_scores_200)
    PageRank_Dict_200 = {k: PageRank_Dict[k] for k in bm25_scores_200 if k in PageRank_Dict}
    # print(first_200_items)
    if len(bm25_scores) > 0 and len(PageRank_Dict_200) > 0:
        combined_scores = combine_bm25_pagerank(bm25_scores, PageRank_Dict_200)
        combined_scores_sorted = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
        return [(str(doc_id), doc_id_to_title_dict[doc_id]) for doc_id, score in combined_scores_sorted[:30]]  # example: top 10 documents
    else:
        return []
